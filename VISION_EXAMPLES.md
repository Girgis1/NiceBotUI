# Vision System - Complete Usage Examples

## üìö Table of Contents

1. [Emergency Stop When Person Detected](#1-emergency-stop-when-person-detected)
2. [Background Subtraction for New Objects](#2-background-subtraction-for-new-objects)
3. [Combined YOLO + Background Subtraction](#3-combined-yolo--background-subtraction)
4. [Inventory Counting](#4-inventory-counting)
5. [Fine-Tuning on Your Products](#5-fine-tuning-on-your-products)
6. [Action Triggers Configuration](#6-action-triggers-configuration)

---

## 1. Emergency Stop When Person Detected üö®

**Use Case:** Safety system that pauses/stops robot when operator approaches.

### Quick Setup (GUI)

1. **Settings ‚Üí Vision Tab**
2. **Enable Hand Detection pipeline** (already exists)
3. Set `Dashboard Indicator: ON`
4. Status bar turns **RED** when person detected

### Advanced Setup (Python API)

```python
from vision_pipelines.beauty_detector import BeautyProductDetector
from vision_pipelines.action_triggers import (
    VisionActionTrigger,
    create_emergency_stop_trigger
)

# Initialize detector
detector = BeautyProductDetector(
    model_size="nano",  # Fast for safety-critical
    confidence_threshold=0.5,
)

# Initialize trigger system
trigger_system = VisionActionTrigger()

# Define emergency stop callback
def emergency_stop(rule, detections):
    print("‚ö†Ô∏è PERSON DETECTED - EMERGENCY STOP!")
    # Your robot stop code here:
    # robot.emergency_stop()
    # or: robot.pause()
    # or: dashboard.trigger_emergency_stop()

# Create trigger rule
rule = create_emergency_stop_trigger(
    on_person_detected=emergency_stop,
    confidence_threshold=0.5,  # 50% confidence
    cooldown_seconds=0.5,      # Check twice per second
)

# Add to trigger system
trigger_system.add_rule(rule)

# In your camera loop
import cv2
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # Run detection
    result = detector.detect(frame)
    
    # Check triggers
    triggered = trigger_system.process_detections(
        result.detections,
        timestamp=time.time()
    )
    
    if triggered:
        print(f"Triggered {len(triggered)} actions")
        # Robot is now stopped!
        break
    
    # Visualize
    annotated = detector.visualize(frame, result)
    cv2.imshow("Safety Monitor", annotated)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

**Performance:** Nano model runs at ~30 FPS on CPU, giving ~33ms response time.

---

## 2. Background Subtraction for New Objects üéØ

**Use Case:** Detect when new product arrives in robot cell, ignoring static background.

### Example: "What's New?" Detection

```python
from vision_pipelines.background_subtractor import BackgroundSubtractor
import cv2
import time

# Initialize background subtractor
bg_sub = BackgroundSubtractor(
    learning_rate=0.001,      # Slow adaptation (for stable background)
    threshold=30,             # Sensitivity
    min_foreground_percent=0.5,  # 0.5% of frame must change
)

cap = cv2.VideoCapture(0)

# Phase 1: Learn background (empty scene)
print("Learning background... Keep scene empty!")
for i in range(30):  # 30 frames to learn
    ret, frame = cap.read()
    if ret:
        bg_sub.learn_background(frame)
    time.sleep(0.1)

print("‚úì Background learned! Place products now.")

# Phase 2: Detect new objects
while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # Detect foreground (new objects)
    result = bg_sub.detect_foreground(frame)
    
    if result.has_new_objects:
        print(f"üÜï NEW OBJECT! Changed: {result.foreground_percent:.1f}%")
        
        # Get bounding box of new object
        bbox = bg_sub.get_foreground_bbox(result.foreground_mask)
        if bbox:
            x, y, w, h = bbox
            print(f"   Location: x={x}, y={y}, size={w}x{h}")
            
            # Draw box on frame
            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
    
    # Visualize
    annotated = bg_sub.visualize(frame, result)
    cv2.imshow("Background Subtraction", annotated)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

**Advantages:**
- **Very fast** (~100 FPS, much faster than YOLO)
- **Ignores static background** (robot arm, table, walls)
- **Works in any lighting** (adaptive)
- **No training needed**

---

## 3. Combined YOLO + Background Subtraction ‚ö°

**Use Case:** Use background subtraction to find WHERE things changed, then run YOLO only on that region (much faster!).

```python
from vision_pipelines.beauty_detector import BeautyProductDetector
from vision_pipelines.background_subtractor import (
    BackgroundSubtractor,
    detect_new_objects_with_yolo
)
import cv2

# Initialize both systems
bg_sub = BackgroundSubtractor()
detector = BeautyProductDetector(model_size="small")

cap = cv2.VideoCapture(0)

# Learn background
print("Learning background...")
for _ in range(30):
    ret, frame = cap.read()
    if ret:
        bg_sub.learn_background(frame)

print("‚úì Ready! Monitoring for new objects...")

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # Combined detection (smart ROI)
    bg_result, yolo_result = detect_new_objects_with_yolo(
        frame,
        bg_sub,
        detector,
        use_roi=True  # Only run YOLO on changed region
    )
    
    if bg_result.has_new_objects:
        print("üÜï NEW OBJECT DETECTED")
        
        if yolo_result and yolo_result.total_detected > 0:
            print(f"   Identified as: {yolo_result.detections[0].class_name}")
            print(f"   Confidence: {yolo_result.detections[0].confidence:.2f}")
            
            # Visualize YOLO detection
            frame = detector.visualize(frame, yolo_result)
        else:
            # Just show background subtraction
            frame = bg_sub.visualize(frame, bg_result)
    
    cv2.imshow("Smart Detection", frame)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

**Performance Boost:**
- **Without ROI:** Run YOLO on full 640x480 frame (~18 FPS)
- **With ROI:** Run YOLO on 200x200 region (~45 FPS, 2.5x faster!)
- **When nothing changed:** Skip YOLO entirely (~100 FPS)

---

## 4. Inventory Counting üì¶

**Use Case:** Count products passing by on conveyor belt.

```python
from vision_pipelines.beauty_detector import BeautyProductDetector
from vision_pipelines.action_triggers import (
    VisionActionTrigger,
    create_inventory_counter
)
import cv2
import time

# Product counter
product_count = {"bottle": 0, "tube": 0, "box": 0}

def on_product_counted(rule, detections):
    """Callback when products detected"""
    for det in detections:
        class_name = det.get('class', det.get('class_name'))
        if class_name in product_count:
            product_count[class_name] += 1
            print(f"‚úì Counted {class_name}: {product_count[class_name]} total")

# Initialize
detector = BeautyProductDetector(model_size="small")
trigger_system = VisionActionTrigger()

# Create counter rule
rule = create_inventory_counter(
    product_classes=["bottle", "tube", "box"],
    on_count_update=on_product_counted
)
trigger_system.add_rule(rule)

# Camera loop
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    result = detector.detect(frame)
    
    # Update counts
    trigger_system.process_detections(
        [
            {
                "class": det.class_name,
                "confidence": det.confidence,
                "box": det.box
            }
            for det in result.detections
        ],
        timestamp=time.time()
    )
    
    # Visualize
    annotated = detector.visualize(frame, result)
    
    # Show counts
    y_offset = 100
    for product, count in product_count.items():
        cv2.putText(
            annotated,
            f"{product}: {count}",
            (10, y_offset),
            cv2.FONT_HERSHEY_SIMPLEX,
            1.0,
            (0, 255, 0),
            2
        )
        y_offset += 40
    
    cv2.imshow("Inventory Counter", annotated)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

print("\nüìä Final Inventory:")
for product, count in product_count.items():
    print(f"  {product}: {count}")

cap.release()
cv2.destroyAllWindows()
```

---

## 5. Fine-Tuning on Your Products üé®

**Complete workflow from capture to deployment.**

### Step 1: Capture Training Data (5-10 minutes)

```python
from vision_pipelines.beauty_detector import BeautyProductDetector
from vision_pipelines.training_capture import TrainingDataCapture
import cv2

# Your product classes
MY_PRODUCTS = [
    "lipstick_red",
    "mascara_waterproof",
    "foundation_beige",
    "compact_powder"
]

# Initialize
detector = BeautyProductDetector(model_size="medium")  # Accurate for labeling
capture = TrainingDataCapture(
    dataset_path="./my_beauty_products",
    class_names=MY_PRODUCTS
)

cap = cv2.VideoCapture(0)
print("Press SPACE to capture, ESC to quit")

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # Run detection (pre-label with generic model)
    result = detector.detect(frame, filter_classes=["bottle", "tube"])
    
    # Visualize
    annotated = detector.visualize(frame, result)
    cv2.putText(
        annotated,
        f"Captured: {capture.capture_count}",
        (10, 30),
        cv2.FONT_HERSHEY_SIMPLEX,
        1.0,
        (0, 255, 0),
        2
    )
    cv2.imshow("Capture", annotated)
    
    key = cv2.waitKey(1) & 0xFF
    if key == ord(' '):  # Space to capture
        filename = capture.capture_from_detector_result(frame, result)
        print(f"‚úì Captured {filename}")
    elif key == 27:  # ESC to quit
        break

cap.release()
cv2.destroyAllWindows()

# Show statistics
stats = capture.get_stats()
print(f"\nüìä Captured {stats['image_count']} images")
print(f"   Dataset: {stats['dataset_path']}")
```

### Step 2: Label Data (10-20 minutes)

```bash
# Install labelImg if needed
pip install labelImg

# Launch labeling tool
labelImg ./my_beauty_products/images ./my_beauty_products/data.yaml

# Instructions:
# 1. Draw box around each product
# 2. Select correct class from dropdown
# 3. Press 'Next' to go to next image
# 4. Repeat for all images
```

### Step 3: Train (15 minutes - 2 hours depending on GPU)

```bash
# Quick test (5 minutes on GPU)
yolo segment train \
    data=./my_beauty_products/data.yaml \
    model=yolo11n-seg.pt \
    epochs=50 \
    imgsz=320 \
    batch=16 \
    name=test_run

# Production training (30 minutes - 1 hour on GPU)
yolo segment train \
    data=./my_beauty_products/data.yaml \
    model=yolo11s-seg.pt \
    epochs=100 \
    imgsz=640 \
    batch=16 \
    device=0 \
    name=beauty_products_v1
```

### Step 4: Use in GUI

```python
# In Vision settings:
# Model: custom
# Custom Model Path: runs/segment/beauty_products_v1/weights/best.pt
# Save Profile

# Or programmatically:
detector = BeautyProductDetector(
    custom_model_path="runs/segment/beauty_products_v1/weights/best.pt"
)

# Now it recognizes YOUR specific products!
```

---

## 6. Action Triggers Configuration ‚öôÔ∏è

**Complete example with multiple triggers.**

```python
from vision_pipelines.beauty_detector import BeautyProductDetector
from vision_pipelines.action_triggers import (
    VisionActionTrigger,
    TriggerRule,
    TriggerAction,
    TriggerCondition,
    create_emergency_stop_trigger,
    create_defect_alert,
)
import cv2
import time

# Initialize
detector = BeautyProductDetector(model_size="small")
trigger_system = VisionActionTrigger()

# Trigger 1: Emergency stop on person
def emergency_stop(rule, detections):
    print("‚ö†Ô∏è EMERGENCY STOP - PERSON DETECTED!")
    # robot.emergency_stop()

trigger_system.add_rule(create_emergency_stop_trigger(
    on_person_detected=emergency_stop,
    confidence_threshold=0.6
))

# Trigger 2: Alert on defect
def defect_alert(rule, detections):
    print("üö® DEFECT DETECTED!")
    # Send alert, log, etc.

trigger_system.add_rule(create_defect_alert(
    defect_class="defect",
    on_defect_found=defect_alert,
    confidence_threshold=0.7
))

# Trigger 3: Custom - start robot when bottle appears
def start_robot_sequence(rule, detections):
    print("ü§ñ Bottle detected - Starting pick sequence")
    # robot.start_pick_sequence()

trigger_system.add_rule(TriggerRule(
    class_names=["bottle"],
    condition=TriggerCondition.DETECTED,
    action=TriggerAction.CUSTOM,
    custom_callback=start_robot_sequence,
    cooldown_seconds=5.0,  # Don't trigger again for 5 seconds
    description="Start pick on bottle detection"
))

# Main loop
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    result = detector.detect(frame)
    
    # Process all triggers
    triggered = trigger_system.process_detections(
        [
            {
                "class": det.class_name,
                "confidence": det.confidence,
                "box": det.box
            }
            for det in result.detections
        ],
        timestamp=time.time()
    )
    
    # Display triggered actions
    for action in triggered:
        print(f"‚úì {action['rule']}: {action['status']}")
    
    # Visualize
    annotated = detector.visualize(frame, result)
    cv2.imshow("Action Triggers", annotated)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

# Print statistics
stats = trigger_system.get_statistics()
print("\nüìä Trigger Statistics:")
for rule in stats['rules']:
    print(f"  {rule['description']}: {rule['trigger_count']} triggers")
```

---

## üéØ Quick Reference

### When to Use What?

| Use Case | Solution | Speed | Accuracy |
|----------|----------|-------|----------|
| Safety (person detection) | YOLO nano | 30 FPS | 90% |
| New object alert | Background subtraction | 100 FPS | 95% |
| Product identification | YOLO small | 20 FPS | 85-95% |
| Inventory counting | YOLO + triggers | 20 FPS | 90% |
| Quality inspection | Custom YOLO | 10 FPS | 95%+ |
| Fast screening | BG Sub + YOLO ROI | 45 FPS | 90% |

### Performance Optimization Tips

1. **Use background subtraction first** - Skip YOLO when nothing changed
2. **ROI detection** - Only run YOLO on changed region
3. **Frame skipping** - Don't process every frame
4. **Model size** - Nano for speed, medium for accuracy
5. **GPU acceleration** - 10x faster with CUDA

---

## üìö Additional Resources

- **Roboflow Universe:** https://universe.roboflow.com/
- **Ultralytics Docs:** https://docs.ultralytics.com/
- **Training Guide:** `VISION_DETECTION_GUIDE.md`
- **API Reference:** `IMPLEMENTATION_SUMMARY.md`

---

**Happy detecting!** ü§ñ‚ú®

